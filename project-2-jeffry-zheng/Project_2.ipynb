{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeffry Zheng Project 2: Tracking User Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial set up included cloning down the project 2 repo before creating and moving to the assignment branch.\n",
    "    \n",
    "    /w205/project-2-jeffry-zheng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling assessment data:\n",
    "    \n",
    "    curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spinning up the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring up and edit YAML file:\n",
    "    \n",
    "    cp ~/w205/course-content/08-Querying-Data/docker-compose.yml ~/w205/project-2-jeffry-zheng/\n",
    "\n",
    "    fix 8888 for spark\n",
    "        expose:\n",
    "    -\t“8888”\n",
    "    ports:\n",
    "    -\t“8888:8888”\n",
    "YAML file was edited to connect Jupyter Notebook to pyspark. \n",
    "YAML file services include zookeeper, kafka, cloudera, spark, and mids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spinning up the cluster:\n",
    "    \n",
    "    docker-compose up -d\n",
    "    docker-compose ps\n",
    "    docker ps -a\n",
    "    docker-compose logs -f kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and checking topic \"assessments\":\n",
    "\n",
    "    docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "    docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish and consume messages with Kafka:\n",
    "\n",
    "    docker-compose exec mids bash -c \"cat /w205/project-2-jeffry-zheng/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "    docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linking the Spark container to the /w205 mount point:\n",
    "\n",
    "    docker-compose exec spark bash\n",
    "    ln -s /w205 w205\n",
    "    exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting a Jupyter Notebook for a pyspark kernel:\n",
    "\n",
    "    docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming messages with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty Print First Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "p = pprint.PrettyPrinter(indent=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"assessment-attempts-20180128-121051-nested.json\",\"r\")\n",
    "s = f.read()\n",
    "json_data = json.loads(s)\n",
    "f.close()\n",
    "len(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_exam_id': '37f0a30a-7464-11e6-aa92-a8667f27e5dc',\n",
      " 'certification': 'false',\n",
      " 'exam_name': 'Normal Forms and All That Jazz Master Class',\n",
      " 'keen_created_at': '1516717442.735266',\n",
      " 'keen_id': '5a6745820eb8ab00016be1f1',\n",
      " 'keen_timestamp': '1516717442.735266',\n",
      " 'max_attempts': '1.0',\n",
      " 'sequences': {'attempt': 1,\n",
      "               'counts': {'all_correct': False,\n",
      "                          'correct': 2,\n",
      "                          'incomplete': 1,\n",
      "                          'incorrect': 1,\n",
      "                          'submitted': 4,\n",
      "                          'total': 4,\n",
      "                          'unanswered': 0},\n",
      "               'id': '5b28a462-7a3b-42e0-b508-09f3906d1703',\n",
      "               'questions': [{'id': '7a2ed6d3-f492-49b3-b8aa-d080a8aad986',\n",
      "                              'options': [{'at': '2018-01-23T14:23:24.670Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '49c574b4-5c82-4ffd-9bd1-c3358faf850d',\n",
      "                                           'submitted': 1},\n",
      "                                          {'at': '2018-01-23T14:23:25.914Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'f2528210-35c3-4320-acf3-9056567ea19f',\n",
      "                                           'submitted': 1},\n",
      "                                          {'checked': False,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'd1bf026f-554f-4543-bdd2-54dcf105b826'}],\n",
      "                              'user_correct': False,\n",
      "                              'user_incomplete': True,\n",
      "                              'user_result': 'missed_some',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': 'bbed4358-999d-4462-9596-bad5173a6ecb',\n",
      "                              'options': [{'at': '2018-01-23T14:23:30.116Z',\n",
      "                                           'checked': True,\n",
      "                                           'id': 'a35d0e80-8c49-415d-b8cb-c21a02627e2b',\n",
      "                                           'submitted': 1},\n",
      "                                          {'checked': False,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'bccd6e2e-2cef-4c72-8bfa-317db0ac48bb'},\n",
      "                                          {'at': '2018-01-23T14:23:41.791Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '7e0b639a-2ef8-4604-b7eb-5018bd81a91b',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': False,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'incorrect',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': 'e6ad8644-96b1-4617-b37b-a263dded202c',\n",
      "                              'options': [{'at': '2018-01-23T14:23:52.510Z',\n",
      "                                           'checked': False,\n",
      "                                           'id': 'a9333679-de9d-41ff-bb3d-b239d6b95732'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '85795acc-b4b1-4510-bd6e-41648a3553c9'},\n",
      "                                          {'at': '2018-01-23T14:23:54.223Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'c185ecdb-48fb-4edb-ae4e-0204ac7a0909',\n",
      "                                           'submitted': 1},\n",
      "                                          {'at': '2018-01-23T14:23:53.862Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '77a66c83-d001-45cd-9a5a-6bba8eb7389e',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': True,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'correct',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': '95194331-ac43-454e-83de-ea8913067055',\n",
      "                              'options': [{'checked': False,\n",
      "                                           'id': '59b9fc4b-f239-4850-b1f9-912d1fd3ca13'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '2c29e8e8-d4a8-406e-9cdf-de28ec5890fe'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '62feee6e-9b76-4123-bd9e-c0b35126b1f1'},\n",
      "                                          {'at': '2018-01-23T14:24:00.807Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '7f13df9c-fcbe-4424-914f-2206f106765c',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': True,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'correct',\n",
      "                              'user_submitted': True}]},\n",
      " 'started_at': '2018-01-23T14:23:19.082Z',\n",
      " 'user_exam_id': '6d4089e4-bde5-4a22-b65f-18bce9ab79c8'}\n"
     ]
    }
   ],
   "source": [
    "p.pprint(json_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrolling Nested json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating data frame by subscribing to the kafka topic\n",
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n",
    "raw_assessments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#passing json data as a string to data frame\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#writing to hdfs\n",
    "assessments.write.parquet(\"/tmp/assessments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- base_exam_id: string (nullable = true)\n",
      " |-- certification: string (nullable = true)\n",
      " |-- exam_name: string (nullable = true)\n",
      " |-- keen_created_at: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- keen_timestamp: string (nullable = true)\n",
      " |-- max_attempts: string (nullable = true)\n",
      " |-- sequences: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: map (containsNull = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: boolean (valueContainsNull = true)\n",
      " |-- started_at: string (nullable = true)\n",
      " |-- user_exam_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a temporary table\n",
    "extracted_assessments.registerTempTable('assessments')\n",
    "extracted_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------------+----+\n",
      "|             keen_id|certification|max_attempts| att|\n",
      "+--------------------+-------------+------------+----+\n",
      "|5a6745820eb8ab000...|        false|         1.0|null|\n",
      "|5a674541ab6b0a000...|        false|         1.0|null|\n",
      "|5a67999d3ed3e3000...|        false|         1.0|null|\n",
      "|5a6799694fc7c7000...|        false|         1.0|null|\n",
      "|5a6791e824fccd000...|        false|         1.0|null|\n",
      "|5a67a0b6852c2a000...|        false|         1.0|null|\n",
      "|5a67b627cc80e6000...|        false|         1.0|null|\n",
      "|5a67ac8cb0a5f4000...|        false|         1.0|null|\n",
      "|5a67a9ba060087000...|        false|         1.0|null|\n",
      "|5a67ac54411aed000...|        false|         1.0|null|\n",
      "+--------------------+-------------+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select keen_id, certification, max_attempts, sequences.att from assessments limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------------------------------------+\n",
      "|    keen_timestamp|sequences[questions] AS `questions`[0][user_incomplete]|\n",
      "+------------------+-------------------------------------------------------+\n",
      "| 1516717442.735266|                                                   true|\n",
      "| 1516717377.639827|                                                  false|\n",
      "| 1516738973.653394|                                                  false|\n",
      "|1516738921.1137421|                                                  false|\n",
      "| 1516737000.212122|                                                  false|\n",
      "| 1516740790.309757|                                                  false|\n",
      "|1516746279.3801291|                                                  false|\n",
      "| 1516743820.305464|                                                  false|\n",
      "|  1516743098.56811|                                                  false|\n",
      "| 1516743764.813107|                                                  false|\n",
      "+------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select keen_timestamp, sequences.questions[0].user_incomplete from assessments limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Question 1: What are the top 5 most popular exams taken?\n",
    "\n",
    "Table of the top 5 exams taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|num_exams|           exam_name|\n",
      "+---------+--------------------+\n",
      "|        2|Being a Better In...|\n",
      "|        2|          Great Bash|\n",
      "|        2|Introduction to P...|\n",
      "|        2|Architectural Con...|\n",
      "|        1|Learning Spring P...|\n",
      "+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(distinct(base_exam_id)) as num_exams, exam_name from assessments group by exam_name order by num_exams desc limit 5\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(num_exams=2, exam_name='Introduction to Python')"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_exams = spark.sql(\"select count(distinct(base_exam_id)) as num_exams, exam_name from assessments group by exam_name order by num_exams desc limit 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_exams.write.parquet(\"/tmp/pop_exams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested multi-value as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lambda_sequences(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"sequences_id\" : raw_dict[\"sequences\"][\"id\"], \"sequences_attempt\" : raw_dict[\"sequences\"][\"attempt\"]}\n",
    "    return Row(**my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sequences = assessments.rdd.map(my_lambda_sequences).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- sequences_attempt: long (nullable = true)\n",
      " |-- sequences_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_sequences.registerTempTable('sequences')\n",
    "my_sequences.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+-----------------+\n",
      "|             keen_id|    keen_timestamp|        sequences_id|sequences_attempt|\n",
      "+--------------------+------------------+--------------------+-----------------+\n",
      "|5a17a67efa1257000...|1511499390.3836269|8ac691f8-8c1a-403...|                1|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|9bd87823-4508-4e0...|                1|\n",
      "|5a29dcac74b662000...|1512692908.8423469|e7110aed-0d08-4cb...|                1|\n",
      "|5a2fdab0eabeda000...|1513085616.2275269|cd800e92-afc3-447...|                1|\n",
      "|5a30105020e9d4000...|1513099344.8624721|8ac691f8-8c1a-403...|                1|\n",
      "|5a3a6fc3f0a100000...| 1513779139.354213|e7110aed-0d08-4cb...|                1|\n",
      "|5a4e17fe08a892000...|1515067390.1336551|9abd5b51-6bd8-11e...|                1|\n",
      "|5a4f3c69cc6444000...| 1515142249.858722|083844c5-772f-48d...|                1|\n",
      "|5a51b21bd0480b000...| 1515303451.773272|e7110aed-0d08-4cb...|                1|\n",
      "|5a575a85329e1a000...| 1515674245.348099|25ca21fe-4dbb-446...|                1|\n",
      "+--------------------+------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.keen_id, a.keen_timestamp, s.sequences_id, s.sequences_attempt from assessments a join sequences s on a.keen_id = s.keen_id order by s.sequences_attempt limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sequence_info = spark.sql(\"select a.keen_id, a.keen_timestamp, s.sequences_id, s.sequences_attempt from assessments a join sequences s on a.keen_id = s.keen_id order by s.sequences_attempt limit 10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sequence_info.write.parquet(\"/tmp/sequence_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Question 2: Which sequences were attempted the most?\n",
    "\n",
    "Table of top distinct sequences by the total number of attempts per sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|        sequences_id|num_attempts|\n",
      "+--------------------+------------+\n",
      "|e7110aed-0d08-4cb...|         394|\n",
      "|066b5326-e547-4da...|         162|\n",
      "|cc7043a6-1511-4f5...|         158|\n",
      "|8ac691f8-8c1a-403...|         154|\n",
      "|9bd87823-4508-4e0...|         128|\n",
      "|cec4308a-64dc-484...|         119|\n",
      "|3585eaaa-512d-4ff...|         109|\n",
      "|25ca21fe-4dbb-446...|          95|\n",
      "|07177166-021f-449...|          85|\n",
      "|cbc07487-e537-4b9...|          80|\n",
      "+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sequences_id, sum(sequences_attempt) as num_attempts from sequences group by sequences_id order by num_attempts desc limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most attempts for a sequence was 394."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested multi-valued as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def my_lambda_questions(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    my_count = 0\n",
    "    for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "        my_count += 1\n",
    "        my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"my_count\" : my_count, \"id\" : l[\"id\"]}\n",
    "        my_list.append(Row(**my_dict))\n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "my_questions = assessments.rdd.flatMap(my_lambda_questions).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- keen_id: string (nullable = true)\n",
      " |-- my_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_questions.registerTempTable('questions')\n",
    "my_questions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|             keen_id|    keen_timestamp|                  id|\n",
      "+--------------------+------------------+--------------------+\n",
      "|5a17a67efa1257000...|1511499390.3836269|803fc93f-7eb2-412...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|f3cb88cc-5b79-41b...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|32fe7d8d-6d89-4db...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|5c34cf19-8cfd-4f5...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|0603e6f4-c3f9-4c2...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|26a06b88-2758-45b...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|25b6effe-79b0-4c4...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|6de03a9b-2a78-46b...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|aaf39991-fa83-470...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|aab2e817-73dc-4ff...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select q.keen_id, a.keen_timestamp, q.id from assessments a join questions q on a.keen_id = q.keen_id limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Question 3: Which questions came up the most on all the exams?\n",
    "\n",
    "Table of question id and the number of times it appeared throughout the exams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                  id|my_count|\n",
      "+--------------------+--------+\n",
      "|8d5372d4-a63b-40c...|      20|\n",
      "|7d527603-ed07-4d1...|      19|\n",
      "|bdc4d043-b161-426...|      18|\n",
      "|2494e12b-070e-458...|      17|\n",
      "|a994e9ca-208a-40a...|      16|\n",
      "|ebc3d26e-ed70-4cd...|      15|\n",
      "|ddac0ade-2320-48d...|      14|\n",
      "|0bc8696a-b9b0-43b...|      13|\n",
      "|c27494a4-fe24-4a1...|      12|\n",
      "|be969a50-0474-409...|      11|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select id, my_count from questions order by my_count desc limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most frequent question appeared 20 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "question_info = spark.sql(\"select id, my_count from questions order by my_count desc limit 10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "question_info.write.parquet(\"/tmp/question_info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling \"holes\" in json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def my_lambda_correct_total(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    if \"sequences\" in raw_dict:  \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:     \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:         \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    return my_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "my_correct_total = assessments.rdd.flatMap(my_lambda_correct_total).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "my_correct_total.registerTempTable('ct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|correct|total|\n",
      "+-------+-----+\n",
      "|      2|    4|\n",
      "|      1|    4|\n",
      "|      3|    4|\n",
      "|      2|    4|\n",
      "|      3|    4|\n",
      "|      5|    5|\n",
      "|      1|    1|\n",
      "|      5|    5|\n",
      "|      4|    4|\n",
      "|      0|    5|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from ct limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|score|\n",
      "+-----+\n",
      "|  0.5|\n",
      "| 0.25|\n",
      "| 0.75|\n",
      "|  0.5|\n",
      "| 0.75|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select correct / total as score from ct limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        avg_score|\n",
      "+-----------------+\n",
      "|62.65699745547047|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select avg(correct / total)*100 as avg_score from ct limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| standard_deviation|\n",
      "+-------------------+\n",
      "|0.31086692286170553|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select stddev(correct / total) as standard_deviation from ct limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking results in hadoop\n",
    "\n",
    "All directories:\n",
    "\n",
    "    docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "Individual directories:\n",
    "\n",
    "    docker-compose exec cloudera hadoop fs -ls /tmp/assessments\n",
    "    docker-compose exec cloudera hadoop fs -ls /tmp/pop_exams\n",
    "    docker-compose exec cloudera hadoop fs -ls /tmp/sequence_info\n",
    "    docker-compose exec cloudera hadoop fs -ls /tmp/question_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shutting down the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    docker-compose down\n",
    "    docker-compose ps\n",
    "    docker ps -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
